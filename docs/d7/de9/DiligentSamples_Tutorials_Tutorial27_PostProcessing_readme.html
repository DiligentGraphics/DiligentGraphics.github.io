<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "https://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" lang="en-US">
<head>
<meta http-equiv="Content-Type" content="text/xhtml;charset=UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=11"/>
<meta name="generator" content="Doxygen 1.13.2"/>
<meta name="viewport" content="width=device-width, initial-scale=1"/>
<title>Diligent Engine: Tutorial27 - Post Processing</title>
<link href="../../tabs.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="../../jquery.js"></script>
<script type="text/javascript" src="../../dynsections.js"></script>
<script type="text/javascript" src="../../clipboard.js"></script>
<link href="../../navtree.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="../../navtreedata.js"></script>
<script type="text/javascript" src="../../navtree.js"></script>
<script type="text/javascript" src="../../resize.js"></script>
<script type="text/javascript" src="../../cookie.js"></script>
<link href="../../search/search.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="../../search/searchdata.js"></script>
<script type="text/javascript" src="../../search/search.js"></script>
<script type="text/javascript">
/* @license magnet:?xt=urn:btih:d3d9a9a6595521f9666a5e94cc830dab83b65699&amp;dn=expat.txt MIT */
  $(function() { init_search(); });
/* @license-end */
</script>
<link href="../../doxygen.css" rel="stylesheet" type="text/css" />
</head>
<body>
<div id="top"><!-- do not remove this div, it is closed by doxygen! -->
<div id="titlearea">
<table cellspacing="0" cellpadding="0">
 <tbody>
 <tr id="projectrow">
  <td id="projectalign">
   <div id="projectname">Diligent Engine
   </div>
  </td>
    <td>        <div id="MSearchBox" class="MSearchBoxInactive">
        <span class="left">
          <span id="MSearchSelect"                onmouseover="return searchBox.OnSearchSelectShow()"                onmouseout="return searchBox.OnSearchSelectHide()">&#160;</span>
          <input type="text" id="MSearchField" value="" placeholder="Search" accesskey="S"
               onfocus="searchBox.OnSearchFieldFocus(true)" 
               onblur="searchBox.OnSearchFieldFocus(false)" 
               onkeyup="searchBox.OnSearchFieldChange(event)"/>
          </span><span class="right">
            <a id="MSearchClose" href="javascript:searchBox.CloseResultsWindow()"><img id="MSearchCloseImg" border="0" src="../../search/close.svg" alt=""/></a>
          </span>
        </div>
</td>
 </tr>
 </tbody>
</table>
</div>
<!-- end header part -->
<!-- Generated by Doxygen 1.13.2 -->
<script type="text/javascript">
/* @license magnet:?xt=urn:btih:d3d9a9a6595521f9666a5e94cc830dab83b65699&amp;dn=expat.txt MIT */
var searchBox = new SearchBox("searchBox", "../../search/",'.html');
/* @license-end */
</script>
<script type="text/javascript">
/* @license magnet:?xt=urn:btih:d3d9a9a6595521f9666a5e94cc830dab83b65699&amp;dn=expat.txt MIT */
$(function() { codefold.init(1); });
/* @license-end */
</script>
</div><!-- top -->
<div id="side-nav" class="ui-resizable side-nav-resizable">
  <div id="nav-tree">
    <div id="nav-tree-contents">
      <div id="nav-sync" class="sync"></div>
    </div>
  </div>
  <div id="splitbar" style="-moz-user-select:none;" 
       class="ui-resizable-handle">
  </div>
</div>
<script type="text/javascript">
/* @license magnet:?xt=urn:btih:d3d9a9a6595521f9666a5e94cc830dab83b65699&amp;dn=expat.txt MIT */
$(function(){initNavTree('d7/de9/DiligentSamples_Tutorials_Tutorial27_PostProcessing_readme.html','../../'); initResizable(true); });
/* @license-end */
</script>
<div id="doc-content">
<!-- window showing the filter options -->
<div id="MSearchSelectWindow"
     onmouseover="return searchBox.OnSearchSelectShow()"
     onmouseout="return searchBox.OnSearchSelectHide()"
     onkeydown="return searchBox.OnSearchSelectKey(event)">
</div>

<!-- iframe showing the search results (closed by default) -->
<div id="MSearchResultsWindow">
<div id="MSearchResults">
<div class="SRPage">
<div id="SRIndex">
<div id="SRResults"></div>
<div class="SRStatus" id="Loading">Loading...</div>
<div class="SRStatus" id="Searching">Searching...</div>
<div class="SRStatus" id="NoMatches">No Matches</div>
</div>
</div>
</div>
</div>

<div><div class="header">
  <div class="headertitle"><div class="title">Tutorial27 - Post Processing</div></div>
</div><!--header-->
<div class="contents">
<div class="textblock"><p>This tutorial demonstrates how to use post-processing effects from the DiligentFX module.</p>
<p><img src="https://github.com/DiligentGraphics/DiligentSamples/raw/master/Tutorials/Tutorial27_PostProcessing/Screenshot.jpg" alt="" class="inline"/></p>
<p><a href="https://diligentgraphics.github.io/wasm-modules/Tutorial27_PostProcessing/Tutorial27_PostProcessing.html">â–¶ Run in the browser</a></p>
<h2><a class="anchor" id="autotoc_md457"></a>
Table of contents</h2>
<ul>
<li>Introduction</li>
<li>Render Passes<ul>
<li>Generation G-Buffer</li>
<li>Computing of SSR</li>
<li>Computing of Lighting</li>
<li>Tone Mapping</li>
</ul>
</li>
<li>Resources</li>
</ul>
<h2><a class="anchor" id="autotoc_md458"></a>
Introduction</h2>
<p>In this tutorial, we demonstrate how you can integrate a post-processing stack into your project. At a high level, the rendering in this demo can be divided into the following parts:</p><ul>
<li>Generating G-Buffer</li>
<li>Computing SSR</li>
<li>Computing Lighting</li>
<li>Tone Mapping</li>
</ul>
<p>We use deferred lighting algorithm to generate images. Initially, we fill a G-Buffer. After that, screen space reflection (SSR) is performed using the previous rendered frame. Following this, we computate lighting. The final phase is tone mapping.</p>
<h2><a class="anchor" id="autotoc_md459"></a>
Render Passes</h2>
<p>In this section, the main steps of rendering are listed.</p>
<h3><a class="anchor" id="autotoc_md460"></a>
Generating G-Buffer</h3>
<p>We use Deferred Shading instead of Forward Rendering because it simplifies the rendering pipeline when using post-effects such as SSAO and SSR. We recommend familiarizing yourself with this tutorial <a href="https://learnopengl.com/Advanced-Lighting/Deferred-Shading"><b>[Learn OpenGL, Deferred Shading]</b></a> if you have not previously encountered the deferred lighting algorithm. For this algorithm, before the lighting calculation stage, we generate a G-Buffer using hybrid ray tracing. For this, we render the AABB (Axis-Aligned Bounding Box) of each object. In the pixel shader, for pixels covered by the AABB in screen space, rays are generated which then search for intersections with the object depending on its geometry type (Sphere, Box). After finding the intersection of the ray with the object, we compute the surface properties and record them in the G-Buffer. If no intersection occurs, we call <code>discard</code> in the pixel shader.</p>
<p>For lighting, we use the PBR (Physically Based Rendering) approach with a Metallic-Roughness Workflow. If you are not familiar with these concepts, we recommend reading this series of articles <a href="https://learnopengl.com/PBR/Theory"><b>[Learn OpenGL, PBR]</b></a>. We use the following G-buffer:</p>
<div class="fragment"><div class="line"><span class="keyword">struct </span>GBuffer</div>
<div class="line">{</div>
<div class="line">    float4 BaseColor    : SV_Target0; <span class="comment">// TEX_FORMAT_RGBA8_UNORM</span></div>
<div class="line">    float2 MaterialData : SV_Target1; <span class="comment">// TEX_FORMAT_RG8_UNORM</span></div>
<div class="line">    float4 Normal       : SV_Target2; <span class="comment">// TEX_FORMAT_RGBA16_FLOAT</span></div>
<div class="line">    float2 Motion       : SV_Target3; <span class="comment">// TEX_FORMAT_RG16_FLOAT</span></div>
<div class="line">    <span class="keywordtype">float</span>  Depth        : SV_Depth;   <span class="comment">// TEX_FORMAT_D32_FLOAT</span></div>
<div class="line">};</div>
</div><!-- fragment --><p>Here is a description of each component.</p>
<table class="markdownTable">
<tr class="markdownTableHead">
<th class="markdownTableHeadNone">Name   </th><th class="markdownTableHeadNone">Description    </th></tr>
<tr class="markdownTableRowOdd">
<td class="markdownTableBodyNone"><code>BaseColor</code>   </td><td class="markdownTableBodyNone"><code>RGB</code> - Base Color of Surface    </td></tr>
<tr class="markdownTableRowEven">
<td class="markdownTableBodyNone"><code>MaterialData</code>   </td><td class="markdownTableBodyNone"><code>R</code> - Roughness, <code>G</code> - Metalness, <code>B</code>- Occlusion    </td></tr>
<tr class="markdownTableRowOdd">
<td class="markdownTableBodyNone"><code>Normal</code>   </td><td class="markdownTableBodyNone"><code>RGB</code> - World Space Normal    </td></tr>
<tr class="markdownTableRowEven">
<td class="markdownTableBodyNone"><code>Motion</code>   </td><td class="markdownTableBodyNone"><code>RG</code> - Motion Vector    </td></tr>
<tr class="markdownTableRowOdd">
<td class="markdownTableBodyNone"><code>Depth</code>   </td><td class="markdownTableBodyNone"><code>D</code> - Depth of surface   </td></tr>
</table>
<p>The G-Buffer generation shader itself looks like this:</p>
<div class="fragment"><div class="line">float2 NormalizedXY = TexUVToNormalizedDeviceXY(VSOut.PixelPosition.xy * g_CurrCamera.f4ViewportSize.zw);</div>
<div class="line">Ray RayWS = CreateCameraRay(NormalizedXY);</div>
<div class="line">Ray RayLS = TransformRayToLocalSpace(RayWS);</div>
<div class="line"> </div>
<div class="line">Intersection Intersect;</div>
<div class="line">Intersect.Distance = -FLT_MAX;</div>
<div class="line">Intersect.Normal = float3(0.0, 0.0, 0.0);</div>
<div class="line"> </div>
<div class="line">switch (g_ObjectAttribs.ObjectType)</div>
<div class="line">{</div>
<div class="line">    case GEOMETRY_TYPE_SPHERE:</div>
<div class="line">        Intersect = IntersectSphere(RayLS, float3(0.0, 0.0, 0.0), 1.0);</div>
<div class="line">        break;</div>
<div class="line">    case GEOMETRY_TYPE_AABB:</div>
<div class="line">        Intersect = IntersectAABB(RayLS, float3(0.0, 0.0, 0.0), float3(1.0, 1.0, 1.0));</div>
<div class="line">        break;</div>
<div class="line">    default:</div>
<div class="line">        break;</div>
<div class="line">}</div>
<div class="line"> </div>
<div class="line">if (Intersect.Distance &lt; RayWS.Min || Intersect.Distance &gt; RayWS.Max)</div>
<div class="line">    discard;</div>
<div class="line"> </div>
<div class="line">float3 PositionLS = RayLS.Origin + Intersect.Distance * RayLS.Direction;</div>
<div class="line">float3 PositionWS = RayWS.Origin + Intersect.Distance * RayWS.Direction;</div>
<div class="line">float3 NormalWS = ComputeNorma(Intersect.Normal);</div>
<div class="line">MaterialAttribs ObjectMaterial = ComputeMaterial(PositionWS);</div>
<div class="line"> </div>
<div class="line">GBuffer Output;</div>
<div class="line">Output.BaseColor = ObjectMaterial.BaseColor;</div>
<div class="line">Output.MaterialData = float2(ObjectMaterial.Roughness, ObjectMaterial.Metalness);</div>
<div class="line">Output.Normal = float4(NormalWS, 1.0);</div>
<div class="line">Output.Motion = ComputeMotion(NormalizedXY, PositionLS);</div>
<div class="line">Output.Depth = ComputeDepth(PositionWS);</div>
</div><!-- fragment --><p>At this stage, we create a primary ray <code>CreateCameraRay</code> from the NDC coordinates and view and projection matrices (see <a href="https://bmrysz.wordpress.com/2020/06/13/how-to-create-correct-ray-tracing-camera/"><b>[Ray Generation, bmrysz]</b></a>). After creating the ray, we transform it into the object's Local Space, and then the ray searches for an intersection with the object depending on its type. If you are interested in how to find the intersection of a ray with a transformed object, we recommend reading this article <a href="https://graphicscompendium.com/raytracing/12-transformations"><b>[Ian Dunn, ZoÃ« Wood, GPC]</b></a>. The results are returned in the <code>Intersect</code> structure, which stores details of the intersection. This structure contains parameters such as the surface normal <code>Normal</code> and <code>Distance</code> parameter, representing the $t$ parameter in the linear equation: $x = x_0 + r * t$ where $x_0$ - origin of ray <code>Ray::Origin</code>, $r$ - direction of ray <code>Ray::Direction</code>, $x$ - intersection point <code>Position</code>. If no intersection occurs, we call <code>discard</code>. Next, we calculate the intersection points in World Space and Local Space (The point in Local Space will be needed in the future for calculating the motion vector). Note that the $t$ parameter is in Local Space, but it can be substituted into the ray equation from World Space, and we will obtain the correct intersection point (you can read why this is the case in the article I mentioned earlier). Next, we transform the obtained normal from Local Space to World Space using the object's normal matrix, and calculate the material of surface (we simply pass the material of the object through a constant buffer).</p>
<p>If an intersection occurs, we record the corresponding parameters of the intersection point in the returned variable <code>Output</code> (which is <code>GBuffer</code> structure we described earlier). One important detail here is the calculation of motion (velocity) vectors, which will be required for the Screen Space Reflections (SSR) computation step. The velocity vector is a vector that contains the speed of each pixel in NDC space. For more details on how to calculate motion vectors, you can read this article <a href="https://www.elopezr.com/temporal-aa-and-the-quest-for-the-holy-trail/"><b>[TAA, elopezr]</b></a> section 'Motion Vectors'. Conceptually, motion vector for the current pixel can be computed in four steps:</p>
<p>1) Calculate the current pixel's $NDC_1$ coordinates. 2) Calculate the current pixel's world position. 3) With the pixel's world position known, calculate the $NDC_0$ using the previous frame view and projection matrices. 4) Knowing the $NDC$ coordinates of both the current and previous frames, we obtain the velocity vector $NDC_1$ - $NDC_0$.</p>
<div class="fragment"><div class="line">float2 ComputeMotion(float2 NDC1, float3 PositionLS)</div>
<div class="line">{</div>
<div class="line">    float4 PositionWS = mul(float4(PositionLS, 1.0), g_ObjectAttribs.PrevWorldTransform);</div>
<div class="line">    float4 NDC0 = mul(PositionWS, g_PrevCamera.mViewProj);</div>
<div class="line">    NDC0.xy = NDC0.xy / NDC0.w;</div>
<div class="line"> </div>
<div class="line">    float2 Motion = NDC1.xy - NDC0.xy;</div>
<div class="line">    return Motion;</div>
<div class="line">}</div>
</div><!-- fragment --><p>From the above code, you can see that our code does not entirely follow the described approach. We pass a position point in Local Space, and then transform it from Local Space back to World Space, but using the object's transformation matrix from the previous frame. This is done to ensure that the motion vectors function correctly when the object moves. We recommend reading the article we mentioned earlier if you are interested in understanding why this is necessary.</p>
<h3><a class="anchor" id="autotoc_md461"></a>
Computing SSR</h3>
<p>In this rendering pass, we compute the SSR. In this rendering pass, we calculate SSR (Screen Space Reflections) using the implementation from the DiligentFX module. I recommend familiarizing yourself with the <a href="https://github.com/DiligentGraphics/DiligentFX/tree/master/PostProcess/ScreenSpaceReflection">documentation</a>. Below is the Ð¡PU-side code necessary to initiate this rendering pass:</p>
<div class="fragment"><div class="line">{</div>
<div class="line">    PostFXContext::FrameDesc FrameDesc;</div>
<div class="line">    FrameDesc.Width  = m_pSwapChain-&gt;GetDesc().Width;</div>
<div class="line">    FrameDesc.Height = m_pSwapChain-&gt;GetDesc().Height;</div>
<div class="line">    FrameDesc.Index  = m_CurrentFrameNumber;</div>
<div class="line">    m_PostFXContext-&gt;PrepareResources(FrameDesc);</div>
<div class="line">    m_ScreenSpaceReflection-&gt;PrepareResources(m_pDevice, m_PostFXContext.get());</div>
<div class="line">}</div>
<div class="line"> </div>
<div class="line">{</div>
<div class="line">    PostFXContext::RenderAttributes PostFXAttibs;</div>
<div class="line">    PostFXAttibs.pDevice          = m_pDevice;</div>
<div class="line">    PostFXAttibs.pDeviceContext   = m_pImmediateContext;</div>
<div class="line">    PostFXAttibs.pCameraAttribsCB = m_Resources[RESOURCE_IDENTIFIER_CAMERA_CONSTANT_BUFFER].AsBuffer();</div>
<div class="line">    m_PostFXContext-&gt;Execute(PostFXAttibs);</div>
<div class="line">}</div>
<div class="line"> </div>
<div class="line">{</div>
<div class="line">    const Uint32 PrevFrameIdx = (m_CurrentFrameNumber + 0x1) &amp; 0x1;</div>
<div class="line"> </div>
<div class="line">    ScreenSpaceReflection::RenderAttributes SSRRenderAttribs{};</div>
<div class="line">    SSRRenderAttribs.pDevice            = m_pDevice;</div>
<div class="line">    SSRRenderAttribs.pDeviceContext     = m_pImmediateContext;</div>
<div class="line">    SSRRenderAttribs.pPostFXContext     = m_PostFXContext.get();</div>
<div class="line">    SSRRenderAttribs.pColorBufferSRV    = m_Resources[RESOURCE_IDENTIFIER_RADIANCE0 + PrevFrameIdx].GetTextureSRV();</div>
<div class="line">    SSRRenderAttribs.pDepthBufferSRV    = m_GBuffer-&gt;GetBuffer(GBUFFER_RT_DEPTH)-&gt;GetDefaultView(TEXTURE_VIEW_DEPTH_STENCIL);</div>
<div class="line">    SSRRenderAttribs.pNormalBufferSRV   = m_GBuffer-&gt;GetBuffer(GBUFFER_RT_NORMAL)-&gt;GetDefaultView(TEXTURE_VIEW_SHADER_RESOURCE);</div>
<div class="line">    SSRRenderAttribs.pMaterialBufferSRV = m_GBuffer-&gt;GetBuffer(GBUFFER_RT_MATERIAL_DATA)-&gt;GetDefaultView(TEXTURE_VIEW_SHADER_RESOURCE);</div>
<div class="line">    SSRRenderAttribs.pMotionVectorsSRV  = m_GBuffer-&gt;GetBuffer(RESOURCE_IDENTIFIER_MOTION_VECTORS)-&gt;GetDefaultView(TEXTURE_VIEW_SHADER_RESOURCE);</div>
<div class="line">    SSRRenderAttribs.pSSRAttribs        = &amp;m_ShaderParams-&gt;SSRSettings;</div>
<div class="line">    SSRRenderAttribs.FeatureFlag        = ScreenSpaceReflection::FEATURE_PREVIOUS_FRAME;</div>
<div class="line">    m_ScreenSpaceReflection-&gt;Execute(SSRRenderAttribs);</div>
<div class="line">}</div>
</div><!-- fragment --><p>There are three stages in the the code snippet above:</p>
<p>1) Resource preparation through the methods <code>PostFXContext::Prepare</code> and <code>ScreenSpaceReflection::Prepare</code>. At this stage, internal resources(such as constant buffers, textures, etc.) are created for each object, if they haven't been already. 2) Calculating the shared data by calling the method <code>PostFXContext::Execute</code>. Here, we compute data shared by all post-effects, such as Blue Noise texture. 3) Direct computation of SSR by calling <code>ScreenSpaceReflection::Execute</code>.</p>
<p>In the SSR computation stage, we pass the necessary data from our G-Buffer to the corresponding members of the <code>ScreenSpaceReflection::RenderAttributes</code> structure. In <code>ColorBufferSRV</code>, we pass the previously rendered frame that is yet to be processed through the tone mapping stage. Additionally, we pass the <code>ScreenSpaceReflection::FEATURE_PREVIOUS_FRAME</code> flag, thus telling the algorithm that we are using the previous frame as input. We input the previous frame into SSR to simplify the rendering pipeline, as it would otherwise require us to create separate render targets for the diffuse and specular components during lighting calculation. Additionally, it would necessitate an extra pass for combining <code>[SSR, SpecularRadiance, DiffuseRadiance]</code>. Also, using the previous frame as the input color buffer for SSR enables multiple bouncing.</p>
<h3><a class="anchor" id="autotoc_md462"></a>
Computing Lighting</h3>
<p>At this stage, we calculate the lighting based on data from the G-Buffer and SSR. We only IBL (Image-Based Lighting) in the demo.</p>
<div class="fragment"><div class="line">SurfaceInformation SurfInfo = ExtractGBuffer(VSOut);</div>
<div class="line">float3 F0 = lerp(float3(0.04, 0.04, 0.04), SurfInfo.BaseColor, SurfInfo.Metallness);</div>
<div class="line">float3 F = FresnelSchlickRoughness(saturate(dot(SurfInfo.Normal, ViewDir)), F0, SurfInfo.Roughness);</div>
<div class="line"> </div>
<div class="line">float3 kS = F;</div>
<div class="line">float3 kD = (float3(1.0, 1.0, 1.0) - kS) * (1.0 - SurfInfo.Metallness);</div>
<div class="line"> </div>
<div class="line">float3 Diffuse = SurfInfo.BaseColor * ComputeDiffuseIBL(SurfInfo.Normal);</div>
<div class="line">float3 Specular = ComputeSpecularIBL(VSOut.f4PixelPos.xy, F0, SurfInfo.Normal, ViewDir, SurfInfo.Roughness);</div>
<div class="line">float3 Radiance = kD * Diffuse + Specular;</div>
</div><!-- fragment --><p>In the code above, we first extract the surface information from the G-Buffer. Then, we calculate the diffuse and specular components of the rendering equation. Let's move to the specular component computation, for which we computed the SSR:</p>
<div class="fragment"><div class="line">float3 ComputeSpecularIBL(float2 Location, float3 F0, float3 N, float3 V, float Roughness)</div>
<div class="line">{</div>
<div class="line">    float NdotV = saturate(dot(N, V));</div>
<div class="line">    float3 R = reflect(-V, N);</div>
<div class="line">    float2 BRDF_LUT = SampleBRDFIntegrationMap(float2(NdotV, Roughness));</div>
<div class="line">    float4 SSR = g_TextureSSR.Load(int3(Location, 0));</div>
<div class="line">    float3 T1 = SamplePrefilteredEnvironmentMap(R, Roughness * g_PBRRendererAttibs.PrefilteredCubeLastMip); </div>
<div class="line">    float3 T2 = (F0 * BRDF_LUT.x + BRDF_LUT.yyy);</div>
<div class="line">    return lerp(T1, SSR.rgb, SSR.w) * T2;</div>
<div class="line">}</div>
</div><!-- fragment --><p>If you find this part of the code challenging to understand, I recommend reading the <a href="https://github.com/DiligentGraphics/DiligentFX/tree/master/PostProcess/ScreenSpaceReflection">documentation</a> in the section 'Implementation Details - Ray Tracing'. We extract the $T1$ and $T2$ sums, which are corresponding values in the Split-Sum Approximation expression. What does each sum represent is explained in detail in the documentation. Subsequently, we perform the interpolation between $T1$ and the value from SSR, by alpha channel of the SSR texture. This alpha channel have value <code>1.0</code> when the SSR's reflected ray intersects with any geometry, and <code>0.0</code> when there's no intersection. Accordingly, we use the value from the Prefiltered Environment Map when no intersection has occurred.</p>
<h3><a class="anchor" id="autotoc_md463"></a>
Tone Mapping</h3>
<p>At this stage, we convert the HDR range to LDR. We use the Uncharted 2 function, passing the corresponding macro <code>TONE_MAPPING_MODE_UNCHARTED2</code> when creating the pixel shader. For more details on the tone mapping process, we recommend reading this article <a href="https://bartwronski.com/2016/09/01/dynamic-range-and-evs/"><b>[Bart Wronski, ToneMap]</b></a>.</p>
<div class="fragment"><div class="line">// g_ToneMappingAttibs.Padding0 - packed white point</div>
<div class="line">float3 HDRColor = g_TextureHDR.Load(int3(VSOut.f4PixelPos.xy, 0));</div>
<div class="line">float3 SDRColor = ToneMap(HDRColor, TMAttribs, g_PBRRendererAttibs.AverageLogLum);</div>
</div><!-- fragment --><h2><a class="anchor" id="autotoc_md464"></a>
Resources</h2>
<ul>
<li><b>[Learn OpenGL, PBR]</b> Theory of Physycal Base Rendering - <a href="https://learnopengl.com/PBR/Theory">https://learnopengl.com/PBR/Theory</a></li>
<li><b>[Learn OpenGL, Deferred Shading]</b> Deferred Shading - <a href="https://learnopengl.com/Advanced-Lighting/Deferred-Shading">https://learnopengl.com/Advanced-Lighting/Deferred-Shading</a></li>
<li><b>[Ray Generation, bmrysz]</b> How to create correct ray tracing camera - <a href="https://bmrysz.wordpress.com/2020/06/13/how-to-create-correct-ray-tracing-camera/">https://bmrysz.wordpress.com/2020/06/13/how-to-create-correct-ray-tracing-camera/</a></li>
<li><b>[TAA, elopezr]</b> Temporal AA and the quest for the Holy Trail - <a href="https://www.elopezr.com/temporal-aa-and-the-quest-for-the-holy-trail/">https://www.elopezr.com/temporal-aa-and-the-quest-for-the-holy-trail/</a></li>
<li><b>[Bart Wronski, ToneMap]</b> Image dynamic range - <a href="https://bartwronski.com/2016/09/01/dynamic-range-and-evs/">https://bartwronski.com/2016/09/01/dynamic-range-and-evs/</a></li>
<li><b>[Ian Dunn, ZoÃ« Wood, GPC]</b> Graphics Programming Compendium Chapter 34: Transformations <a href="https://graphicscompendium.com/raytracing/12-transformations">https://graphicscompendium.com/raytracing/12-transformations</a> </li>
</ul>
</div></div><!-- contents -->
</div><!-- PageDoc -->
</div><!-- doc-content -->
<!-- HTML footer for doxygen 1.13.2-->
<!-- start footer part -->
<div id="nav-path" class="navpath"><!-- id is needed for treeview function! -->
  <ul>
    <li class="footer">
      <a href="https://diligentgraphics.com">
        <img class="footer" src="https://github.com/DiligentGraphics/DiligentCore/raw/master/media/diligentgraphics-logo.png" width="99" height="32" alt="Diligent Graphics" />
      </a>
    </li>
  </ul>
</div>
</body>
</html>
